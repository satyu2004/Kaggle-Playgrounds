{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47d2f16b",
   "metadata": {},
   "source": [
    "Validate the idea of composition. Use LGB inside XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dbb3765",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "params_lgb = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": -1,\n",
    "    \"verbose\": -1,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "params_xgb = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    # \"max_depth\": 6,\n",
    "    \"verbosity\": 0,\n",
    "    \"seed\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448fd20a",
   "metadata": {},
   "source": [
    "# 2-layer model attempt (has data leak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280aebd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[551]\tvalid_0's rmse: 0.0598906\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list.append() takes exactly one argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Fuse the train and val sets\u001b[39;00m\n\u001b[0;32m     56\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([X_train, X_val], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), pd\u001b[38;5;241m.\u001b[39mconcat([y_train, y_val], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 57\u001b[0m \u001b[43mpredictors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mz_lgb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mz_hgb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Split the data into train and vals sets again\u001b[39;00m\n\u001b[0;32m     60\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(X_train, y_train, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m43\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: list.append() takes exactly one argument (2 given)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "enc = LabelEncoder()\n",
    "predictors = ['Sex', 'Age', 'Height', 'Weight', 'Duration', 'Heart_Rate',\n",
    "       'Body_Temp']\n",
    "\n",
    "target = 'z'\n",
    "\n",
    "\n",
    "\n",
    "# Prepare the train data\n",
    "train = pd.read_csv('train.csv')\n",
    "train['Sex'] = enc.fit_transform(train['Sex'])\n",
    "train['z'] = np.log1p(train['Calories'])\n",
    "X = train[predictors]\n",
    "z = train[target]\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, z, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# LGB\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
    "# Enforce early stopping\n",
    "callbacks = [lgb.early_stopping(stopping_rounds=10, verbose=1)]\n",
    "lgb_model = lgb.train(params_lgb, lgb_train, num_boost_round=1000, valid_sets=lgb_val, callbacks=callbacks)\n",
    "X_train['z_lgb'] = lgb_model.predict(X_train, num_iteration=lgb_model.best_iteration)\n",
    "X_val['z_lgb'] = lgb_model.predict(X_val, num_iteration=lgb_model.best_iteration)\n",
    "X_test['z_lgb'] = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\n",
    "\n",
    "# HGB\n",
    "hgb = HistGradientBoostingRegressor(max_iter=1000,\n",
    "                                    learning_rate=0.05,\n",
    "                                    random_state=42, \n",
    "                                    early_stopping=True,\n",
    "                                    validation_fraction=0.1,\n",
    "                                    n_iter_no_change=10\n",
    ")\n",
    "\n",
    "hgb.fit(X_train, y_train)\n",
    "X_train['z_hgb'] = hgb.predict(X_train)\n",
    "X_val['z_hgb'] = hgb.predict(X_val)\n",
    "X_test['z_hgb'] = hgb.predict(X_test)\n",
    "\n",
    "\n",
    "# Fuse the train and val sets\n",
    "X_train, y_train = pd.concat([X_train, X_val], axis=0), pd.concat([y_train, y_val], axis=0)\n",
    "predictors.append('z_lgb', 'z_hgb')\n",
    "\n",
    "# Split the data into train and vals sets again\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=43)\n",
    "\n",
    "# XGB\n",
    "xgb_train = xgb.DMatrix(X_train, label=y_train)\n",
    "xgb_val = xgb.DMatrix(X_val, label=y_val)\n",
    "xgb_model = xgb.train(params_xgb, xgb_train, num_boost_round=1000, evals=[(xgb_val, 'val')], early_stopping_rounds=100)\n",
    "\n",
    "z_pred = xgb_model.predict(xgb.DMatrix(X_test), iteration_range=(0, xgb_model.best_iteration))\n",
    "\n",
    "# Measure the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, z_pred))\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d264a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tval-rmse:0.91256\n",
      "[1]\tval-rmse:0.86715\n",
      "[2]\tval-rmse:0.82402\n",
      "[3]\tval-rmse:0.78305\n",
      "[4]\tval-rmse:0.74415\n",
      "[5]\tval-rmse:0.70720\n",
      "[6]\tval-rmse:0.67210\n",
      "[7]\tval-rmse:0.63878\n",
      "[8]\tval-rmse:0.60714\n",
      "[9]\tval-rmse:0.57708\n",
      "[10]\tval-rmse:0.54855\n",
      "[11]\tval-rmse:0.52146\n",
      "[12]\tval-rmse:0.49575\n",
      "[13]\tval-rmse:0.47133\n",
      "[14]\tval-rmse:0.44815\n",
      "[15]\tval-rmse:0.42616\n",
      "[16]\tval-rmse:0.40528\n",
      "[17]\tval-rmse:0.38546\n",
      "[18]\tval-rmse:0.36666\n",
      "[19]\tval-rmse:0.34882\n",
      "[20]\tval-rmse:0.33190\n",
      "[21]\tval-rmse:0.31584\n",
      "[22]\tval-rmse:0.30062\n",
      "[23]\tval-rmse:0.28619\n",
      "[24]\tval-rmse:0.27251\n",
      "[25]\tval-rmse:0.25954\n",
      "[26]\tval-rmse:0.24726\n",
      "[27]\tval-rmse:0.23562\n",
      "[28]\tval-rmse:0.22460\n",
      "[29]\tval-rmse:0.21417\n",
      "[30]\tval-rmse:0.20430\n",
      "[31]\tval-rmse:0.19496\n",
      "[32]\tval-rmse:0.18613\n",
      "[33]\tval-rmse:0.17778\n",
      "[34]\tval-rmse:0.16990\n",
      "[35]\tval-rmse:0.16245\n",
      "[36]\tval-rmse:0.15542\n",
      "[37]\tval-rmse:0.14880\n",
      "[38]\tval-rmse:0.14256\n",
      "[39]\tval-rmse:0.13668\n",
      "[40]\tval-rmse:0.13114\n",
      "[41]\tval-rmse:0.12593\n",
      "[42]\tval-rmse:0.12104\n",
      "[43]\tval-rmse:0.11645\n",
      "[44]\tval-rmse:0.11215\n",
      "[45]\tval-rmse:0.10811\n",
      "[46]\tval-rmse:0.10434\n",
      "[47]\tval-rmse:0.10082\n",
      "[48]\tval-rmse:0.09752\n",
      "[49]\tval-rmse:0.09445\n",
      "[50]\tval-rmse:0.09159\n",
      "[51]\tval-rmse:0.08892\n",
      "[52]\tval-rmse:0.08644\n",
      "[53]\tval-rmse:0.08415\n",
      "[54]\tval-rmse:0.08202\n",
      "[55]\tval-rmse:0.08005\n",
      "[56]\tval-rmse:0.07824\n",
      "[57]\tval-rmse:0.07655\n",
      "[58]\tval-rmse:0.07499\n",
      "[59]\tval-rmse:0.07357\n",
      "[60]\tval-rmse:0.07225\n",
      "[61]\tval-rmse:0.07104\n",
      "[62]\tval-rmse:0.06994\n",
      "[63]\tval-rmse:0.06892\n",
      "[64]\tval-rmse:0.06799\n",
      "[65]\tval-rmse:0.06714\n",
      "[66]\tval-rmse:0.06636\n",
      "[67]\tval-rmse:0.06565\n",
      "[68]\tval-rmse:0.06500\n",
      "[69]\tval-rmse:0.06441\n",
      "[70]\tval-rmse:0.06387\n",
      "[71]\tval-rmse:0.06338\n",
      "[72]\tval-rmse:0.06292\n",
      "[73]\tval-rmse:0.06252\n",
      "[74]\tval-rmse:0.06214\n",
      "[75]\tval-rmse:0.06180\n",
      "[76]\tval-rmse:0.06150\n",
      "[77]\tval-rmse:0.06123\n",
      "[78]\tval-rmse:0.06097\n",
      "[79]\tval-rmse:0.06075\n",
      "[80]\tval-rmse:0.06054\n",
      "[81]\tval-rmse:0.06036\n",
      "[82]\tval-rmse:0.06019\n",
      "[83]\tval-rmse:0.06003\n",
      "[84]\tval-rmse:0.05989\n",
      "[85]\tval-rmse:0.05977\n",
      "[86]\tval-rmse:0.05964\n",
      "[87]\tval-rmse:0.05954\n",
      "[88]\tval-rmse:0.05945\n",
      "[89]\tval-rmse:0.05936\n",
      "[90]\tval-rmse:0.05928\n",
      "[91]\tval-rmse:0.05920\n",
      "[92]\tval-rmse:0.05914\n",
      "[93]\tval-rmse:0.05908\n",
      "[94]\tval-rmse:0.05904\n",
      "[95]\tval-rmse:0.05899\n",
      "[96]\tval-rmse:0.05894\n",
      "[97]\tval-rmse:0.05891\n",
      "[98]\tval-rmse:0.05887\n",
      "[99]\tval-rmse:0.05883\n",
      "[100]\tval-rmse:0.05880\n",
      "[101]\tval-rmse:0.05877\n",
      "[102]\tval-rmse:0.05875\n",
      "[103]\tval-rmse:0.05872\n",
      "[104]\tval-rmse:0.05870\n",
      "[105]\tval-rmse:0.05868\n",
      "[106]\tval-rmse:0.05866\n",
      "[107]\tval-rmse:0.05864\n",
      "[108]\tval-rmse:0.05862\n",
      "[109]\tval-rmse:0.05860\n",
      "[110]\tval-rmse:0.05859\n",
      "[111]\tval-rmse:0.05858\n",
      "[112]\tval-rmse:0.05857\n",
      "[113]\tval-rmse:0.05856\n",
      "[114]\tval-rmse:0.05855\n",
      "[115]\tval-rmse:0.05854\n",
      "[116]\tval-rmse:0.05853\n",
      "[117]\tval-rmse:0.05852\n",
      "[118]\tval-rmse:0.05852\n",
      "[119]\tval-rmse:0.05851\n",
      "[120]\tval-rmse:0.05851\n",
      "[121]\tval-rmse:0.05850\n",
      "[122]\tval-rmse:0.05850\n",
      "[123]\tval-rmse:0.05850\n",
      "[124]\tval-rmse:0.05849\n",
      "[125]\tval-rmse:0.05848\n",
      "[126]\tval-rmse:0.05848\n",
      "[127]\tval-rmse:0.05848\n",
      "[128]\tval-rmse:0.05847\n",
      "[129]\tval-rmse:0.05847\n",
      "[130]\tval-rmse:0.05846\n",
      "[131]\tval-rmse:0.05846\n",
      "[132]\tval-rmse:0.05845\n",
      "[133]\tval-rmse:0.05844\n",
      "[134]\tval-rmse:0.05844\n",
      "[135]\tval-rmse:0.05843\n",
      "[136]\tval-rmse:0.05842\n",
      "[137]\tval-rmse:0.05842\n",
      "[138]\tval-rmse:0.05841\n",
      "[139]\tval-rmse:0.05841\n",
      "[140]\tval-rmse:0.05841\n",
      "[141]\tval-rmse:0.05841\n",
      "[142]\tval-rmse:0.05841\n",
      "[143]\tval-rmse:0.05840\n",
      "[144]\tval-rmse:0.05840\n",
      "[145]\tval-rmse:0.05840\n",
      "[146]\tval-rmse:0.05840\n",
      "[147]\tval-rmse:0.05840\n",
      "[148]\tval-rmse:0.05840\n",
      "[149]\tval-rmse:0.05840\n",
      "[150]\tval-rmse:0.05840\n",
      "[151]\tval-rmse:0.05840\n",
      "[152]\tval-rmse:0.05840\n",
      "[153]\tval-rmse:0.05839\n",
      "[154]\tval-rmse:0.05839\n",
      "[155]\tval-rmse:0.05839\n",
      "[156]\tval-rmse:0.05839\n",
      "[157]\tval-rmse:0.05839\n",
      "[158]\tval-rmse:0.05839\n",
      "[159]\tval-rmse:0.05839\n",
      "[160]\tval-rmse:0.05839\n",
      "[161]\tval-rmse:0.05839\n",
      "[162]\tval-rmse:0.05839\n",
      "[163]\tval-rmse:0.05839\n",
      "[164]\tval-rmse:0.05839\n",
      "[165]\tval-rmse:0.05839\n",
      "[166]\tval-rmse:0.05839\n",
      "[167]\tval-rmse:0.05839\n",
      "[168]\tval-rmse:0.05839\n",
      "[169]\tval-rmse:0.05840\n",
      "[170]\tval-rmse:0.05839\n",
      "[171]\tval-rmse:0.05839\n",
      "[172]\tval-rmse:0.05840\n",
      "[173]\tval-rmse:0.05839\n",
      "[174]\tval-rmse:0.05839\n",
      "[175]\tval-rmse:0.05839\n",
      "[176]\tval-rmse:0.05839\n",
      "[177]\tval-rmse:0.05839\n",
      "[178]\tval-rmse:0.05839\n",
      "[179]\tval-rmse:0.05839\n",
      "[180]\tval-rmse:0.05840\n",
      "[181]\tval-rmse:0.05839\n",
      "[182]\tval-rmse:0.05839\n",
      "[183]\tval-rmse:0.05839\n",
      "[184]\tval-rmse:0.05839\n",
      "[185]\tval-rmse:0.05839\n",
      "[186]\tval-rmse:0.05839\n",
      "[187]\tval-rmse:0.05839\n",
      "[188]\tval-rmse:0.05839\n",
      "[189]\tval-rmse:0.05839\n",
      "[190]\tval-rmse:0.05839\n",
      "[191]\tval-rmse:0.05839\n",
      "[192]\tval-rmse:0.05839\n",
      "[193]\tval-rmse:0.05839\n",
      "[194]\tval-rmse:0.05839\n",
      "[195]\tval-rmse:0.05839\n",
      "[196]\tval-rmse:0.05839\n",
      "[197]\tval-rmse:0.05839\n",
      "[198]\tval-rmse:0.05839\n",
      "[199]\tval-rmse:0.05839\n",
      "[200]\tval-rmse:0.05839\n",
      "[201]\tval-rmse:0.05839\n",
      "[202]\tval-rmse:0.05839\n",
      "[203]\tval-rmse:0.05839\n",
      "[204]\tval-rmse:0.05840\n",
      "[205]\tval-rmse:0.05840\n",
      "[206]\tval-rmse:0.05840\n",
      "[207]\tval-rmse:0.05840\n",
      "[208]\tval-rmse:0.05839\n",
      "[209]\tval-rmse:0.05840\n",
      "[210]\tval-rmse:0.05839\n",
      "[211]\tval-rmse:0.05839\n",
      "[212]\tval-rmse:0.05839\n",
      "[213]\tval-rmse:0.05839\n",
      "[214]\tval-rmse:0.05839\n",
      "[215]\tval-rmse:0.05839\n",
      "[216]\tval-rmse:0.05839\n",
      "[217]\tval-rmse:0.05839\n",
      "[218]\tval-rmse:0.05839\n",
      "[219]\tval-rmse:0.05839\n",
      "[220]\tval-rmse:0.05839\n",
      "[221]\tval-rmse:0.05839\n",
      "[222]\tval-rmse:0.05839\n",
      "[223]\tval-rmse:0.05839\n",
      "[224]\tval-rmse:0.05839\n",
      "[225]\tval-rmse:0.05839\n",
      "[226]\tval-rmse:0.05839\n",
      "[227]\tval-rmse:0.05839\n",
      "[228]\tval-rmse:0.05839\n",
      "[229]\tval-rmse:0.05839\n",
      "[230]\tval-rmse:0.05839\n",
      "[231]\tval-rmse:0.05840\n",
      "[232]\tval-rmse:0.05840\n",
      "[233]\tval-rmse:0.05840\n",
      "[234]\tval-rmse:0.05840\n",
      "[235]\tval-rmse:0.05840\n",
      "[236]\tval-rmse:0.05840\n",
      "[237]\tval-rmse:0.05840\n",
      "[238]\tval-rmse:0.05840\n",
      "[239]\tval-rmse:0.05840\n",
      "[240]\tval-rmse:0.05840\n",
      "[241]\tval-rmse:0.05840\n",
      "[242]\tval-rmse:0.05840\n",
      "[243]\tval-rmse:0.05840\n",
      "[244]\tval-rmse:0.05840\n",
      "[245]\tval-rmse:0.05840\n",
      "[246]\tval-rmse:0.05840\n",
      "[247]\tval-rmse:0.05840\n",
      "[248]\tval-rmse:0.05839\n",
      "[249]\tval-rmse:0.05840\n",
      "[250]\tval-rmse:0.05839\n",
      "[251]\tval-rmse:0.05839\n",
      "[252]\tval-rmse:0.05839\n",
      "[253]\tval-rmse:0.05839\n",
      "[254]\tval-rmse:0.05839\n",
      "[255]\tval-rmse:0.05839\n",
      "[256]\tval-rmse:0.05839\n",
      "[257]\tval-rmse:0.05839\n",
      "[258]\tval-rmse:0.05838\n",
      "[259]\tval-rmse:0.05839\n",
      "[260]\tval-rmse:0.05839\n",
      "[261]\tval-rmse:0.05839\n",
      "[262]\tval-rmse:0.05839\n",
      "[263]\tval-rmse:0.05838\n",
      "[264]\tval-rmse:0.05839\n",
      "[265]\tval-rmse:0.05838\n",
      "[266]\tval-rmse:0.05838\n",
      "[267]\tval-rmse:0.05838\n",
      "[268]\tval-rmse:0.05838\n",
      "[269]\tval-rmse:0.05838\n",
      "[270]\tval-rmse:0.05838\n",
      "[271]\tval-rmse:0.05838\n",
      "[272]\tval-rmse:0.05838\n",
      "[273]\tval-rmse:0.05838\n",
      "[274]\tval-rmse:0.05838\n",
      "[275]\tval-rmse:0.05838\n",
      "[276]\tval-rmse:0.05837\n",
      "[277]\tval-rmse:0.05838\n",
      "[278]\tval-rmse:0.05837\n",
      "[279]\tval-rmse:0.05837\n",
      "[280]\tval-rmse:0.05837\n",
      "[281]\tval-rmse:0.05837\n",
      "[282]\tval-rmse:0.05837\n",
      "[283]\tval-rmse:0.05837\n",
      "[284]\tval-rmse:0.05837\n",
      "[285]\tval-rmse:0.05837\n",
      "[286]\tval-rmse:0.05837\n",
      "[287]\tval-rmse:0.05837\n",
      "[288]\tval-rmse:0.05837\n",
      "[289]\tval-rmse:0.05837\n",
      "[290]\tval-rmse:0.05837\n",
      "[291]\tval-rmse:0.05837\n",
      "[292]\tval-rmse:0.05838\n",
      "[293]\tval-rmse:0.05837\n",
      "[294]\tval-rmse:0.05837\n",
      "[295]\tval-rmse:0.05837\n",
      "[296]\tval-rmse:0.05838\n",
      "[297]\tval-rmse:0.05837\n",
      "[298]\tval-rmse:0.05837\n",
      "[299]\tval-rmse:0.05838\n",
      "[300]\tval-rmse:0.05838\n",
      "[301]\tval-rmse:0.05838\n",
      "[302]\tval-rmse:0.05838\n",
      "[303]\tval-rmse:0.05838\n",
      "[304]\tval-rmse:0.05838\n",
      "[305]\tval-rmse:0.05838\n",
      "[306]\tval-rmse:0.05838\n",
      "[307]\tval-rmse:0.05838\n",
      "[308]\tval-rmse:0.05838\n",
      "[309]\tval-rmse:0.05838\n",
      "[310]\tval-rmse:0.05838\n",
      "[311]\tval-rmse:0.05838\n",
      "[312]\tval-rmse:0.05838\n",
      "[313]\tval-rmse:0.05838\n",
      "[314]\tval-rmse:0.05838\n",
      "[315]\tval-rmse:0.05838\n",
      "[316]\tval-rmse:0.05838\n",
      "[317]\tval-rmse:0.05838\n",
      "[318]\tval-rmse:0.05838\n",
      "[319]\tval-rmse:0.05837\n",
      "[320]\tval-rmse:0.05837\n",
      "[321]\tval-rmse:0.05837\n",
      "[322]\tval-rmse:0.05837\n",
      "[323]\tval-rmse:0.05837\n",
      "[324]\tval-rmse:0.05837\n",
      "[325]\tval-rmse:0.05837\n",
      "[326]\tval-rmse:0.05837\n",
      "[327]\tval-rmse:0.05837\n",
      "[328]\tval-rmse:0.05837\n",
      "[329]\tval-rmse:0.05836\n",
      "[330]\tval-rmse:0.05836\n",
      "[331]\tval-rmse:0.05836\n",
      "[332]\tval-rmse:0.05836\n",
      "[333]\tval-rmse:0.05837\n",
      "[334]\tval-rmse:0.05837\n",
      "[335]\tval-rmse:0.05836\n",
      "[336]\tval-rmse:0.05836\n",
      "[337]\tval-rmse:0.05836\n",
      "[338]\tval-rmse:0.05836\n",
      "[339]\tval-rmse:0.05836\n",
      "[340]\tval-rmse:0.05836\n",
      "[341]\tval-rmse:0.05836\n",
      "[342]\tval-rmse:0.05836\n",
      "[343]\tval-rmse:0.05835\n",
      "[344]\tval-rmse:0.05836\n",
      "[345]\tval-rmse:0.05836\n",
      "[346]\tval-rmse:0.05836\n",
      "[347]\tval-rmse:0.05836\n",
      "[348]\tval-rmse:0.05835\n",
      "[349]\tval-rmse:0.05836\n",
      "[350]\tval-rmse:0.05836\n",
      "[351]\tval-rmse:0.05835\n",
      "[352]\tval-rmse:0.05835\n",
      "[353]\tval-rmse:0.05835\n",
      "[354]\tval-rmse:0.05835\n",
      "[355]\tval-rmse:0.05834\n",
      "[356]\tval-rmse:0.05834\n",
      "[357]\tval-rmse:0.05835\n",
      "[358]\tval-rmse:0.05835\n",
      "[359]\tval-rmse:0.05835\n",
      "[360]\tval-rmse:0.05835\n",
      "[361]\tval-rmse:0.05834\n",
      "[362]\tval-rmse:0.05834\n",
      "[363]\tval-rmse:0.05834\n",
      "[364]\tval-rmse:0.05834\n",
      "[365]\tval-rmse:0.05835\n",
      "[366]\tval-rmse:0.05834\n",
      "[367]\tval-rmse:0.05834\n",
      "[368]\tval-rmse:0.05835\n",
      "[369]\tval-rmse:0.05834\n",
      "[370]\tval-rmse:0.05834\n",
      "[371]\tval-rmse:0.05834\n",
      "[372]\tval-rmse:0.05834\n",
      "[373]\tval-rmse:0.05834\n",
      "[374]\tval-rmse:0.05834\n",
      "[375]\tval-rmse:0.05834\n",
      "[376]\tval-rmse:0.05834\n",
      "[377]\tval-rmse:0.05834\n",
      "[378]\tval-rmse:0.05834\n",
      "[379]\tval-rmse:0.05834\n",
      "[380]\tval-rmse:0.05834\n",
      "[381]\tval-rmse:0.05834\n",
      "[382]\tval-rmse:0.05834\n",
      "[383]\tval-rmse:0.05834\n",
      "[384]\tval-rmse:0.05834\n",
      "[385]\tval-rmse:0.05834\n",
      "[386]\tval-rmse:0.05834\n",
      "[387]\tval-rmse:0.05834\n",
      "[388]\tval-rmse:0.05835\n",
      "[389]\tval-rmse:0.05835\n",
      "[390]\tval-rmse:0.05835\n",
      "[391]\tval-rmse:0.05835\n",
      "[392]\tval-rmse:0.05835\n",
      "[393]\tval-rmse:0.05835\n",
      "[394]\tval-rmse:0.05835\n",
      "[395]\tval-rmse:0.05835\n",
      "[396]\tval-rmse:0.05835\n",
      "[397]\tval-rmse:0.05835\n",
      "[398]\tval-rmse:0.05835\n",
      "[399]\tval-rmse:0.05835\n",
      "[400]\tval-rmse:0.05835\n",
      "[401]\tval-rmse:0.05835\n",
      "[402]\tval-rmse:0.05835\n",
      "[403]\tval-rmse:0.05835\n",
      "[404]\tval-rmse:0.05835\n",
      "[405]\tval-rmse:0.05835\n",
      "[406]\tval-rmse:0.05835\n",
      "[407]\tval-rmse:0.05835\n",
      "[408]\tval-rmse:0.05835\n",
      "[409]\tval-rmse:0.05835\n",
      "[410]\tval-rmse:0.05835\n",
      "[411]\tval-rmse:0.05835\n",
      "[412]\tval-rmse:0.05835\n",
      "[413]\tval-rmse:0.05835\n",
      "[414]\tval-rmse:0.05835\n",
      "[415]\tval-rmse:0.05835\n",
      "[416]\tval-rmse:0.05835\n",
      "[417]\tval-rmse:0.05835\n",
      "[418]\tval-rmse:0.05835\n",
      "[419]\tval-rmse:0.05835\n",
      "[420]\tval-rmse:0.05835\n",
      "[421]\tval-rmse:0.05835\n",
      "[422]\tval-rmse:0.05835\n",
      "[423]\tval-rmse:0.05835\n",
      "[424]\tval-rmse:0.05835\n",
      "[425]\tval-rmse:0.05835\n",
      "[426]\tval-rmse:0.05836\n",
      "[427]\tval-rmse:0.05836\n",
      "[428]\tval-rmse:0.05836\n",
      "[429]\tval-rmse:0.05836\n",
      "[430]\tval-rmse:0.05836\n",
      "[431]\tval-rmse:0.05836\n",
      "[432]\tval-rmse:0.05836\n",
      "[433]\tval-rmse:0.05836\n",
      "[434]\tval-rmse:0.05836\n",
      "[435]\tval-rmse:0.05836\n",
      "[436]\tval-rmse:0.05836\n",
      "[437]\tval-rmse:0.05836\n",
      "[438]\tval-rmse:0.05836\n",
      "[439]\tval-rmse:0.05836\n",
      "[440]\tval-rmse:0.05836\n",
      "[441]\tval-rmse:0.05836\n",
      "[442]\tval-rmse:0.05836\n",
      "[443]\tval-rmse:0.05836\n",
      "[444]\tval-rmse:0.05837\n",
      "[445]\tval-rmse:0.05837\n",
      "[446]\tval-rmse:0.05836\n",
      "[447]\tval-rmse:0.05837\n",
      "[448]\tval-rmse:0.05837\n",
      "[449]\tval-rmse:0.05837\n",
      "[450]\tval-rmse:0.05837\n",
      "[451]\tval-rmse:0.05837\n",
      "[452]\tval-rmse:0.05837\n",
      "[453]\tval-rmse:0.05837\n",
      "[454]\tval-rmse:0.05837\n",
      "[455]\tval-rmse:0.05837\n",
      "[456]\tval-rmse:0.05837\n",
      "[457]\tval-rmse:0.05837\n",
      "[458]\tval-rmse:0.05837\n",
      "[459]\tval-rmse:0.05837\n",
      "[460]\tval-rmse:0.05837\n",
      "[461]\tval-rmse:0.05837\n",
      "[462]\tval-rmse:0.05837\n",
      "[463]\tval-rmse:0.05837\n",
      "[464]\tval-rmse:0.05837\n",
      "[465]\tval-rmse:0.05837\n",
      "[466]\tval-rmse:0.05837\n",
      "[467]\tval-rmse:0.05837\n",
      "[468]\tval-rmse:0.05837\n",
      "[469]\tval-rmse:0.05838\n",
      "[470]\tval-rmse:0.05838\n",
      "[471]\tval-rmse:0.05837\n",
      "[472]\tval-rmse:0.05837\n",
      "[473]\tval-rmse:0.05837\n",
      "[474]\tval-rmse:0.05837\n",
      "[475]\tval-rmse:0.05837\n",
      "[476]\tval-rmse:0.05837\n",
      "[477]\tval-rmse:0.05838\n",
      "[478]\tval-rmse:0.05838\n",
      "[479]\tval-rmse:0.05838\n",
      "RMSE: 0.0609\n"
     ]
    }
   ],
   "source": [
    "predictors += ['z_lgb', 'z_hgb']\n",
    "\n",
    "# Split the data into train and vals sets again\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=43)\n",
    "\n",
    "# XGB\n",
    "xgb_train = xgb.DMatrix(X_train, label=y_train)\n",
    "xgb_val = xgb.DMatrix(X_val, label=y_val)\n",
    "xgb_model = xgb.train(params_xgb, xgb_train, num_boost_round=1000, evals=[(xgb_val, 'val')], early_stopping_rounds=100)\n",
    "\n",
    "z_pred = xgb_model.predict(xgb.DMatrix(X_test), iteration_range=(0, xgb_model.best_iteration))\n",
    "\n",
    "# Measure the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, z_pred))\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc007a0",
   "metadata": {},
   "source": [
    "# 2-layer (with oof to prevent data leak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18934525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tval-rmse:0.91454\n",
      "[1]\tval-rmse:0.86986\n",
      "[2]\tval-rmse:0.82743\n",
      "[3]\tval-rmse:0.78711\n",
      "[4]\tval-rmse:0.74888\n",
      "[5]\tval-rmse:0.71256\n",
      "[6]\tval-rmse:0.67809\n",
      "[7]\tval-rmse:0.64532\n",
      "[8]\tval-rmse:0.61424\n",
      "[9]\tval-rmse:0.58470\n",
      "[10]\tval-rmse:0.55666\n",
      "[11]\tval-rmse:0.53003\n",
      "[12]\tval-rmse:0.50476\n",
      "[13]\tval-rmse:0.48076\n",
      "[14]\tval-rmse:0.45802\n",
      "[15]\tval-rmse:0.43642\n",
      "[16]\tval-rmse:0.41594\n",
      "[17]\tval-rmse:0.39653\n",
      "[18]\tval-rmse:0.37804\n",
      "[19]\tval-rmse:0.36054\n",
      "[20]\tval-rmse:0.34394\n",
      "[21]\tval-rmse:0.32817\n",
      "[22]\tval-rmse:0.31322\n",
      "[23]\tval-rmse:0.29907\n",
      "[24]\tval-rmse:0.28565\n",
      "[25]\tval-rmse:0.27291\n",
      "[26]\tval-rmse:0.26085\n",
      "[27]\tval-rmse:0.24940\n",
      "[28]\tval-rmse:0.23855\n",
      "[29]\tval-rmse:0.22828\n",
      "[30]\tval-rmse:0.21858\n",
      "[31]\tval-rmse:0.20942\n",
      "[32]\tval-rmse:0.20072\n",
      "[33]\tval-rmse:0.19250\n",
      "[34]\tval-rmse:0.18472\n",
      "[35]\tval-rmse:0.17740\n",
      "[36]\tval-rmse:0.17043\n",
      "[37]\tval-rmse:0.16390\n",
      "[38]\tval-rmse:0.15770\n",
      "[39]\tval-rmse:0.15186\n",
      "[40]\tval-rmse:0.14637\n",
      "[41]\tval-rmse:0.14116\n",
      "[42]\tval-rmse:0.13625\n",
      "[43]\tval-rmse:0.13159\n",
      "[44]\tval-rmse:0.12717\n",
      "[45]\tval-rmse:0.12304\n",
      "[46]\tval-rmse:0.11920\n",
      "[47]\tval-rmse:0.11559\n",
      "[48]\tval-rmse:0.11210\n",
      "[49]\tval-rmse:0.10894\n",
      "[50]\tval-rmse:0.10593\n",
      "[51]\tval-rmse:0.10307\n",
      "[52]\tval-rmse:0.10044\n",
      "[53]\tval-rmse:0.09793\n",
      "[54]\tval-rmse:0.09558\n",
      "[55]\tval-rmse:0.09342\n",
      "[56]\tval-rmse:0.09139\n",
      "[57]\tval-rmse:0.08952\n",
      "[58]\tval-rmse:0.08771\n",
      "[59]\tval-rmse:0.08609\n",
      "[60]\tval-rmse:0.08449\n",
      "[61]\tval-rmse:0.08305\n",
      "[62]\tval-rmse:0.08173\n",
      "[63]\tval-rmse:0.08047\n",
      "[64]\tval-rmse:0.07930\n",
      "[65]\tval-rmse:0.07821\n",
      "[66]\tval-rmse:0.07716\n",
      "[67]\tval-rmse:0.07620\n",
      "[68]\tval-rmse:0.07531\n",
      "[69]\tval-rmse:0.07449\n",
      "[70]\tval-rmse:0.07376\n",
      "[71]\tval-rmse:0.07303\n",
      "[72]\tval-rmse:0.07240\n",
      "[73]\tval-rmse:0.07177\n",
      "[74]\tval-rmse:0.07116\n",
      "[75]\tval-rmse:0.07061\n",
      "[76]\tval-rmse:0.07010\n",
      "[77]\tval-rmse:0.06960\n",
      "[78]\tval-rmse:0.06918\n",
      "[79]\tval-rmse:0.06878\n",
      "[80]\tval-rmse:0.06840\n",
      "[81]\tval-rmse:0.06804\n",
      "[82]\tval-rmse:0.06772\n",
      "[83]\tval-rmse:0.06739\n",
      "[84]\tval-rmse:0.06712\n",
      "[85]\tval-rmse:0.06683\n",
      "[86]\tval-rmse:0.06657\n",
      "[87]\tval-rmse:0.06631\n",
      "[88]\tval-rmse:0.06611\n",
      "[89]\tval-rmse:0.06588\n",
      "[90]\tval-rmse:0.06565\n",
      "[91]\tval-rmse:0.06549\n",
      "[92]\tval-rmse:0.06526\n",
      "[93]\tval-rmse:0.06508\n",
      "[94]\tval-rmse:0.06491\n",
      "[95]\tval-rmse:0.06476\n",
      "[96]\tval-rmse:0.06461\n",
      "[97]\tval-rmse:0.06450\n",
      "[98]\tval-rmse:0.06437\n",
      "[99]\tval-rmse:0.06426\n",
      "[100]\tval-rmse:0.06419\n",
      "[101]\tval-rmse:0.06406\n",
      "[102]\tval-rmse:0.06395\n",
      "[103]\tval-rmse:0.06383\n",
      "[104]\tval-rmse:0.06371\n",
      "[105]\tval-rmse:0.06365\n",
      "[106]\tval-rmse:0.06357\n",
      "[107]\tval-rmse:0.06351\n",
      "[108]\tval-rmse:0.06344\n",
      "[109]\tval-rmse:0.06338\n",
      "[110]\tval-rmse:0.06331\n",
      "[111]\tval-rmse:0.06325\n",
      "[112]\tval-rmse:0.06319\n",
      "[113]\tval-rmse:0.06313\n",
      "[114]\tval-rmse:0.06307\n",
      "[115]\tval-rmse:0.06301\n",
      "[116]\tval-rmse:0.06298\n",
      "[117]\tval-rmse:0.06291\n",
      "[118]\tval-rmse:0.06287\n",
      "[119]\tval-rmse:0.06284\n",
      "[120]\tval-rmse:0.06280\n",
      "[121]\tval-rmse:0.06275\n",
      "[122]\tval-rmse:0.06273\n",
      "[123]\tval-rmse:0.06270\n",
      "[124]\tval-rmse:0.06265\n",
      "[125]\tval-rmse:0.06259\n",
      "[126]\tval-rmse:0.06257\n",
      "[127]\tval-rmse:0.06254\n",
      "[128]\tval-rmse:0.06252\n",
      "[129]\tval-rmse:0.06250\n",
      "[130]\tval-rmse:0.06248\n",
      "[131]\tval-rmse:0.06246\n",
      "[132]\tval-rmse:0.06242\n",
      "[133]\tval-rmse:0.06240\n",
      "[134]\tval-rmse:0.06238\n",
      "[135]\tval-rmse:0.06237\n",
      "[136]\tval-rmse:0.06235\n",
      "[137]\tval-rmse:0.06230\n",
      "[138]\tval-rmse:0.06229\n",
      "[139]\tval-rmse:0.06228\n",
      "[140]\tval-rmse:0.06225\n",
      "[141]\tval-rmse:0.06223\n",
      "[142]\tval-rmse:0.06221\n",
      "[143]\tval-rmse:0.06219\n",
      "[144]\tval-rmse:0.06218\n",
      "[145]\tval-rmse:0.06216\n",
      "[146]\tval-rmse:0.06215\n",
      "[147]\tval-rmse:0.06214\n",
      "[148]\tval-rmse:0.06211\n",
      "[149]\tval-rmse:0.06208\n",
      "[150]\tval-rmse:0.06207\n",
      "[151]\tval-rmse:0.06206\n",
      "[152]\tval-rmse:0.06205\n",
      "[153]\tval-rmse:0.06205\n",
      "[154]\tval-rmse:0.06204\n",
      "[155]\tval-rmse:0.06203\n",
      "[156]\tval-rmse:0.06199\n",
      "[157]\tval-rmse:0.06199\n",
      "[158]\tval-rmse:0.06196\n",
      "[159]\tval-rmse:0.06196\n",
      "[160]\tval-rmse:0.06193\n",
      "[161]\tval-rmse:0.06193\n",
      "[162]\tval-rmse:0.06191\n",
      "[163]\tval-rmse:0.06190\n",
      "[164]\tval-rmse:0.06189\n",
      "[165]\tval-rmse:0.06189\n",
      "[166]\tval-rmse:0.06188\n",
      "[167]\tval-rmse:0.06187\n",
      "[168]\tval-rmse:0.06186\n",
      "[169]\tval-rmse:0.06184\n",
      "[170]\tval-rmse:0.06183\n",
      "[171]\tval-rmse:0.06183\n",
      "[172]\tval-rmse:0.06182\n",
      "[173]\tval-rmse:0.06181\n",
      "[174]\tval-rmse:0.06181\n",
      "[175]\tval-rmse:0.06181\n",
      "[176]\tval-rmse:0.06180\n",
      "[177]\tval-rmse:0.06179\n",
      "[178]\tval-rmse:0.06179\n",
      "[179]\tval-rmse:0.06178\n",
      "[180]\tval-rmse:0.06177\n",
      "[181]\tval-rmse:0.06177\n",
      "[182]\tval-rmse:0.06174\n",
      "[183]\tval-rmse:0.06173\n",
      "[184]\tval-rmse:0.06172\n",
      "[185]\tval-rmse:0.06171\n",
      "[186]\tval-rmse:0.06171\n",
      "[187]\tval-rmse:0.06170\n",
      "[188]\tval-rmse:0.06170\n",
      "[189]\tval-rmse:0.06170\n",
      "[190]\tval-rmse:0.06168\n",
      "[191]\tval-rmse:0.06168\n",
      "[192]\tval-rmse:0.06167\n",
      "[193]\tval-rmse:0.06166\n",
      "[194]\tval-rmse:0.06166\n",
      "[195]\tval-rmse:0.06165\n",
      "[196]\tval-rmse:0.06164\n",
      "[197]\tval-rmse:0.06164\n",
      "[198]\tval-rmse:0.06164\n",
      "[199]\tval-rmse:0.06163\n",
      "[200]\tval-rmse:0.06163\n",
      "[201]\tval-rmse:0.06161\n",
      "[202]\tval-rmse:0.06160\n",
      "[203]\tval-rmse:0.06160\n",
      "[204]\tval-rmse:0.06159\n",
      "[205]\tval-rmse:0.06159\n",
      "[206]\tval-rmse:0.06158\n",
      "[207]\tval-rmse:0.06158\n",
      "[208]\tval-rmse:0.06157\n",
      "[209]\tval-rmse:0.06156\n",
      "[210]\tval-rmse:0.06156\n",
      "[211]\tval-rmse:0.06155\n",
      "[212]\tval-rmse:0.06155\n",
      "[213]\tval-rmse:0.06154\n",
      "[214]\tval-rmse:0.06153\n",
      "[215]\tval-rmse:0.06152\n",
      "[216]\tval-rmse:0.06151\n",
      "[217]\tval-rmse:0.06151\n",
      "[218]\tval-rmse:0.06149\n",
      "[219]\tval-rmse:0.06149\n",
      "[220]\tval-rmse:0.06149\n",
      "[221]\tval-rmse:0.06149\n",
      "[222]\tval-rmse:0.06148\n",
      "[223]\tval-rmse:0.06148\n",
      "[224]\tval-rmse:0.06147\n",
      "[225]\tval-rmse:0.06145\n",
      "[226]\tval-rmse:0.06144\n",
      "[227]\tval-rmse:0.06144\n",
      "[228]\tval-rmse:0.06143\n",
      "[229]\tval-rmse:0.06143\n",
      "[230]\tval-rmse:0.06142\n",
      "[231]\tval-rmse:0.06141\n",
      "[232]\tval-rmse:0.06141\n",
      "[233]\tval-rmse:0.06141\n",
      "[234]\tval-rmse:0.06141\n",
      "[235]\tval-rmse:0.06141\n",
      "[236]\tval-rmse:0.06140\n",
      "[237]\tval-rmse:0.06139\n",
      "[238]\tval-rmse:0.06138\n",
      "[239]\tval-rmse:0.06138\n",
      "[240]\tval-rmse:0.06137\n",
      "[241]\tval-rmse:0.06137\n",
      "[242]\tval-rmse:0.06137\n",
      "[243]\tval-rmse:0.06136\n",
      "[244]\tval-rmse:0.06136\n",
      "[245]\tval-rmse:0.06136\n",
      "[246]\tval-rmse:0.06136\n",
      "[247]\tval-rmse:0.06134\n",
      "[248]\tval-rmse:0.06134\n",
      "[249]\tval-rmse:0.06133\n",
      "[250]\tval-rmse:0.06133\n",
      "[251]\tval-rmse:0.06133\n",
      "[252]\tval-rmse:0.06133\n",
      "[253]\tval-rmse:0.06132\n",
      "[254]\tval-rmse:0.06131\n",
      "[255]\tval-rmse:0.06131\n",
      "[256]\tval-rmse:0.06130\n",
      "[257]\tval-rmse:0.06130\n",
      "[258]\tval-rmse:0.06129\n",
      "[259]\tval-rmse:0.06129\n",
      "[260]\tval-rmse:0.06129\n",
      "[261]\tval-rmse:0.06128\n",
      "[262]\tval-rmse:0.06128\n",
      "[263]\tval-rmse:0.06128\n",
      "[264]\tval-rmse:0.06128\n",
      "[265]\tval-rmse:0.06128\n",
      "[266]\tval-rmse:0.06127\n",
      "[267]\tval-rmse:0.06127\n",
      "[268]\tval-rmse:0.06127\n",
      "[269]\tval-rmse:0.06127\n",
      "[270]\tval-rmse:0.06127\n",
      "[271]\tval-rmse:0.06127\n",
      "[272]\tval-rmse:0.06126\n",
      "[273]\tval-rmse:0.06126\n",
      "[274]\tval-rmse:0.06125\n",
      "[275]\tval-rmse:0.06125\n",
      "[276]\tval-rmse:0.06125\n",
      "[277]\tval-rmse:0.06124\n",
      "[278]\tval-rmse:0.06124\n",
      "[279]\tval-rmse:0.06124\n",
      "[280]\tval-rmse:0.06124\n",
      "[281]\tval-rmse:0.06124\n",
      "[282]\tval-rmse:0.06124\n",
      "[283]\tval-rmse:0.06123\n",
      "[284]\tval-rmse:0.06123\n",
      "[285]\tval-rmse:0.06123\n",
      "[286]\tval-rmse:0.06122\n",
      "[287]\tval-rmse:0.06122\n",
      "[288]\tval-rmse:0.06122\n",
      "[289]\tval-rmse:0.06122\n",
      "[290]\tval-rmse:0.06123\n",
      "[291]\tval-rmse:0.06123\n",
      "[292]\tval-rmse:0.06122\n",
      "[293]\tval-rmse:0.06121\n",
      "[294]\tval-rmse:0.06121\n",
      "[295]\tval-rmse:0.06121\n",
      "[296]\tval-rmse:0.06120\n",
      "[297]\tval-rmse:0.06120\n",
      "[298]\tval-rmse:0.06120\n",
      "[299]\tval-rmse:0.06120\n",
      "[300]\tval-rmse:0.06120\n",
      "[301]\tval-rmse:0.06119\n",
      "[302]\tval-rmse:0.06119\n",
      "[303]\tval-rmse:0.06118\n",
      "[304]\tval-rmse:0.06118\n",
      "[305]\tval-rmse:0.06118\n",
      "[306]\tval-rmse:0.06116\n",
      "[307]\tval-rmse:0.06116\n",
      "[308]\tval-rmse:0.06115\n",
      "[309]\tval-rmse:0.06115\n",
      "[310]\tval-rmse:0.06114\n",
      "[311]\tval-rmse:0.06115\n",
      "[312]\tval-rmse:0.06114\n",
      "[313]\tval-rmse:0.06114\n",
      "[314]\tval-rmse:0.06113\n",
      "[315]\tval-rmse:0.06113\n",
      "[316]\tval-rmse:0.06112\n",
      "[317]\tval-rmse:0.06112\n",
      "[318]\tval-rmse:0.06112\n",
      "[319]\tval-rmse:0.06112\n",
      "[320]\tval-rmse:0.06112\n",
      "[321]\tval-rmse:0.06111\n",
      "[322]\tval-rmse:0.06110\n",
      "[323]\tval-rmse:0.06110\n",
      "[324]\tval-rmse:0.06111\n",
      "[325]\tval-rmse:0.06110\n",
      "[326]\tval-rmse:0.06109\n",
      "[327]\tval-rmse:0.06109\n",
      "[328]\tval-rmse:0.06109\n",
      "[329]\tval-rmse:0.06108\n",
      "[330]\tval-rmse:0.06108\n",
      "[331]\tval-rmse:0.06106\n",
      "[332]\tval-rmse:0.06106\n",
      "[333]\tval-rmse:0.06106\n",
      "[334]\tval-rmse:0.06105\n",
      "[335]\tval-rmse:0.06105\n",
      "[336]\tval-rmse:0.06105\n",
      "[337]\tval-rmse:0.06105\n",
      "[338]\tval-rmse:0.06105\n",
      "[339]\tval-rmse:0.06105\n",
      "[340]\tval-rmse:0.06105\n",
      "[341]\tval-rmse:0.06105\n",
      "[342]\tval-rmse:0.06105\n",
      "[343]\tval-rmse:0.06105\n",
      "[344]\tval-rmse:0.06105\n",
      "[345]\tval-rmse:0.06105\n",
      "[346]\tval-rmse:0.06105\n",
      "[347]\tval-rmse:0.06104\n",
      "[348]\tval-rmse:0.06104\n",
      "[349]\tval-rmse:0.06104\n",
      "[350]\tval-rmse:0.06103\n",
      "[351]\tval-rmse:0.06103\n",
      "[352]\tval-rmse:0.06102\n",
      "[353]\tval-rmse:0.06102\n",
      "[354]\tval-rmse:0.06103\n",
      "[355]\tval-rmse:0.06102\n",
      "[356]\tval-rmse:0.06102\n",
      "[357]\tval-rmse:0.06102\n",
      "[358]\tval-rmse:0.06102\n",
      "[359]\tval-rmse:0.06102\n",
      "[360]\tval-rmse:0.06102\n",
      "[361]\tval-rmse:0.06102\n",
      "[362]\tval-rmse:0.06102\n",
      "[363]\tval-rmse:0.06102\n",
      "[364]\tval-rmse:0.06102\n",
      "[365]\tval-rmse:0.06102\n",
      "[366]\tval-rmse:0.06102\n",
      "[367]\tval-rmse:0.06102\n",
      "[368]\tval-rmse:0.06102\n",
      "[369]\tval-rmse:0.06102\n",
      "[370]\tval-rmse:0.06101\n",
      "[371]\tval-rmse:0.06101\n",
      "[372]\tval-rmse:0.06101\n",
      "[373]\tval-rmse:0.06101\n",
      "[374]\tval-rmse:0.06101\n",
      "[375]\tval-rmse:0.06101\n",
      "[376]\tval-rmse:0.06101\n",
      "[377]\tval-rmse:0.06101\n",
      "[378]\tval-rmse:0.06102\n",
      "[379]\tval-rmse:0.06102\n",
      "[380]\tval-rmse:0.06102\n",
      "[381]\tval-rmse:0.06102\n",
      "[382]\tval-rmse:0.06102\n",
      "[383]\tval-rmse:0.06102\n",
      "[384]\tval-rmse:0.06102\n",
      "[385]\tval-rmse:0.06102\n",
      "[386]\tval-rmse:0.06102\n",
      "[387]\tval-rmse:0.06101\n",
      "[388]\tval-rmse:0.06101\n",
      "[389]\tval-rmse:0.06101\n",
      "[390]\tval-rmse:0.06101\n",
      "[391]\tval-rmse:0.06101\n",
      "[392]\tval-rmse:0.06101\n",
      "[393]\tval-rmse:0.06100\n",
      "[394]\tval-rmse:0.06100\n",
      "[395]\tval-rmse:0.06100\n",
      "[396]\tval-rmse:0.06100\n",
      "[397]\tval-rmse:0.06100\n",
      "[398]\tval-rmse:0.06100\n",
      "[399]\tval-rmse:0.06100\n",
      "[400]\tval-rmse:0.06099\n",
      "[401]\tval-rmse:0.06099\n",
      "[402]\tval-rmse:0.06099\n",
      "[403]\tval-rmse:0.06099\n",
      "[404]\tval-rmse:0.06099\n",
      "[405]\tval-rmse:0.06099\n",
      "[406]\tval-rmse:0.06099\n",
      "[407]\tval-rmse:0.06099\n",
      "[408]\tval-rmse:0.06099\n",
      "[409]\tval-rmse:0.06099\n",
      "[410]\tval-rmse:0.06099\n",
      "[411]\tval-rmse:0.06098\n",
      "[412]\tval-rmse:0.06098\n",
      "[413]\tval-rmse:0.06098\n",
      "[414]\tval-rmse:0.06098\n",
      "[415]\tval-rmse:0.06098\n",
      "[416]\tval-rmse:0.06098\n",
      "[417]\tval-rmse:0.06098\n",
      "[418]\tval-rmse:0.06098\n",
      "[419]\tval-rmse:0.06098\n",
      "[420]\tval-rmse:0.06097\n",
      "[421]\tval-rmse:0.06097\n",
      "[422]\tval-rmse:0.06097\n",
      "[423]\tval-rmse:0.06097\n",
      "[424]\tval-rmse:0.06097\n",
      "[425]\tval-rmse:0.06097\n",
      "[426]\tval-rmse:0.06097\n",
      "[427]\tval-rmse:0.06097\n",
      "[428]\tval-rmse:0.06097\n",
      "[429]\tval-rmse:0.06097\n",
      "[430]\tval-rmse:0.06097\n",
      "[431]\tval-rmse:0.06097\n",
      "[432]\tval-rmse:0.06097\n",
      "[433]\tval-rmse:0.06097\n",
      "[434]\tval-rmse:0.06096\n",
      "[435]\tval-rmse:0.06096\n",
      "[436]\tval-rmse:0.06097\n",
      "[437]\tval-rmse:0.06096\n",
      "[438]\tval-rmse:0.06096\n",
      "[439]\tval-rmse:0.06097\n",
      "[440]\tval-rmse:0.06096\n",
      "[441]\tval-rmse:0.06097\n",
      "[442]\tval-rmse:0.06096\n",
      "[443]\tval-rmse:0.06096\n",
      "[444]\tval-rmse:0.06096\n",
      "[445]\tval-rmse:0.06096\n",
      "[446]\tval-rmse:0.06096\n",
      "[447]\tval-rmse:0.06096\n",
      "[448]\tval-rmse:0.06096\n",
      "[449]\tval-rmse:0.06096\n",
      "[450]\tval-rmse:0.06096\n",
      "[451]\tval-rmse:0.06096\n",
      "[452]\tval-rmse:0.06096\n",
      "[453]\tval-rmse:0.06096\n",
      "[454]\tval-rmse:0.06096\n",
      "[455]\tval-rmse:0.06096\n",
      "[456]\tval-rmse:0.06096\n",
      "[457]\tval-rmse:0.06095\n",
      "[458]\tval-rmse:0.06095\n",
      "[459]\tval-rmse:0.06096\n",
      "[460]\tval-rmse:0.06095\n",
      "[461]\tval-rmse:0.06094\n",
      "[462]\tval-rmse:0.06094\n",
      "[463]\tval-rmse:0.06094\n",
      "[464]\tval-rmse:0.06094\n",
      "[465]\tval-rmse:0.06094\n",
      "[466]\tval-rmse:0.06094\n",
      "[467]\tval-rmse:0.06094\n",
      "[468]\tval-rmse:0.06094\n",
      "[469]\tval-rmse:0.06094\n",
      "[470]\tval-rmse:0.06094\n",
      "[471]\tval-rmse:0.06094\n",
      "[472]\tval-rmse:0.06093\n",
      "[473]\tval-rmse:0.06094\n",
      "[474]\tval-rmse:0.06093\n",
      "[475]\tval-rmse:0.06093\n",
      "[476]\tval-rmse:0.06093\n",
      "[477]\tval-rmse:0.06093\n",
      "[478]\tval-rmse:0.06093\n",
      "[479]\tval-rmse:0.06092\n",
      "[480]\tval-rmse:0.06092\n",
      "[481]\tval-rmse:0.06092\n",
      "[482]\tval-rmse:0.06092\n",
      "[483]\tval-rmse:0.06091\n",
      "[484]\tval-rmse:0.06091\n",
      "[485]\tval-rmse:0.06091\n",
      "[486]\tval-rmse:0.06091\n",
      "[487]\tval-rmse:0.06091\n",
      "[488]\tval-rmse:0.06091\n",
      "[489]\tval-rmse:0.06091\n",
      "[490]\tval-rmse:0.06091\n",
      "[491]\tval-rmse:0.06091\n",
      "[492]\tval-rmse:0.06091\n",
      "[493]\tval-rmse:0.06091\n",
      "[494]\tval-rmse:0.06091\n",
      "[495]\tval-rmse:0.06090\n",
      "[496]\tval-rmse:0.06090\n",
      "[497]\tval-rmse:0.06090\n",
      "[498]\tval-rmse:0.06090\n",
      "[499]\tval-rmse:0.06089\n",
      "[500]\tval-rmse:0.06089\n",
      "[501]\tval-rmse:0.06089\n",
      "[502]\tval-rmse:0.06089\n",
      "[503]\tval-rmse:0.06089\n",
      "[504]\tval-rmse:0.06089\n",
      "[505]\tval-rmse:0.06089\n",
      "[506]\tval-rmse:0.06089\n",
      "[507]\tval-rmse:0.06089\n",
      "[508]\tval-rmse:0.06089\n",
      "[509]\tval-rmse:0.06089\n",
      "[510]\tval-rmse:0.06089\n",
      "[511]\tval-rmse:0.06089\n",
      "[512]\tval-rmse:0.06089\n",
      "[513]\tval-rmse:0.06089\n",
      "[514]\tval-rmse:0.06088\n",
      "[515]\tval-rmse:0.06088\n",
      "[516]\tval-rmse:0.06088\n",
      "[517]\tval-rmse:0.06088\n",
      "[518]\tval-rmse:0.06088\n",
      "[519]\tval-rmse:0.06088\n",
      "[520]\tval-rmse:0.06088\n",
      "[521]\tval-rmse:0.06088\n",
      "[522]\tval-rmse:0.06088\n",
      "[523]\tval-rmse:0.06088\n",
      "[524]\tval-rmse:0.06088\n",
      "[525]\tval-rmse:0.06088\n",
      "[526]\tval-rmse:0.06088\n",
      "[527]\tval-rmse:0.06088\n",
      "[528]\tval-rmse:0.06088\n",
      "[529]\tval-rmse:0.06088\n",
      "[530]\tval-rmse:0.06088\n",
      "[531]\tval-rmse:0.06088\n",
      "[532]\tval-rmse:0.06087\n",
      "[533]\tval-rmse:0.06087\n",
      "[534]\tval-rmse:0.06087\n",
      "[535]\tval-rmse:0.06087\n",
      "[536]\tval-rmse:0.06086\n",
      "[537]\tval-rmse:0.06086\n",
      "[538]\tval-rmse:0.06086\n",
      "[539]\tval-rmse:0.06086\n",
      "[540]\tval-rmse:0.06086\n",
      "[541]\tval-rmse:0.06085\n",
      "[542]\tval-rmse:0.06086\n",
      "[543]\tval-rmse:0.06086\n",
      "[544]\tval-rmse:0.06086\n",
      "[545]\tval-rmse:0.06086\n",
      "[546]\tval-rmse:0.06086\n",
      "[547]\tval-rmse:0.06086\n",
      "[548]\tval-rmse:0.06086\n",
      "[549]\tval-rmse:0.06086\n",
      "[550]\tval-rmse:0.06086\n",
      "[551]\tval-rmse:0.06086\n",
      "[552]\tval-rmse:0.06086\n",
      "[553]\tval-rmse:0.06085\n",
      "[554]\tval-rmse:0.06085\n",
      "[555]\tval-rmse:0.06086\n",
      "[556]\tval-rmse:0.06085\n",
      "[557]\tval-rmse:0.06085\n",
      "[558]\tval-rmse:0.06085\n",
      "[559]\tval-rmse:0.06085\n",
      "[560]\tval-rmse:0.06084\n",
      "[561]\tval-rmse:0.06084\n",
      "[562]\tval-rmse:0.06084\n",
      "[563]\tval-rmse:0.06084\n",
      "[564]\tval-rmse:0.06083\n",
      "[565]\tval-rmse:0.06083\n",
      "[566]\tval-rmse:0.06083\n",
      "[567]\tval-rmse:0.06083\n",
      "[568]\tval-rmse:0.06083\n",
      "[569]\tval-rmse:0.06083\n",
      "[570]\tval-rmse:0.06083\n",
      "[571]\tval-rmse:0.06082\n",
      "[572]\tval-rmse:0.06082\n",
      "[573]\tval-rmse:0.06082\n",
      "[574]\tval-rmse:0.06082\n",
      "[575]\tval-rmse:0.06081\n",
      "[576]\tval-rmse:0.06081\n",
      "[577]\tval-rmse:0.06081\n",
      "[578]\tval-rmse:0.06081\n",
      "[579]\tval-rmse:0.06081\n",
      "[580]\tval-rmse:0.06080\n",
      "[581]\tval-rmse:0.06080\n",
      "[582]\tval-rmse:0.06080\n",
      "[583]\tval-rmse:0.06080\n",
      "[584]\tval-rmse:0.06080\n",
      "[585]\tval-rmse:0.06080\n",
      "[586]\tval-rmse:0.06080\n",
      "[587]\tval-rmse:0.06080\n",
      "[588]\tval-rmse:0.06080\n",
      "[589]\tval-rmse:0.06080\n",
      "[590]\tval-rmse:0.06080\n",
      "[591]\tval-rmse:0.06080\n",
      "[592]\tval-rmse:0.06080\n",
      "[593]\tval-rmse:0.06080\n",
      "[594]\tval-rmse:0.06079\n",
      "[595]\tval-rmse:0.06079\n",
      "[596]\tval-rmse:0.06079\n",
      "[597]\tval-rmse:0.06079\n",
      "[598]\tval-rmse:0.06079\n",
      "[599]\tval-rmse:0.06079\n",
      "[600]\tval-rmse:0.06079\n",
      "[601]\tval-rmse:0.06079\n",
      "[602]\tval-rmse:0.06079\n",
      "[603]\tval-rmse:0.06079\n",
      "[604]\tval-rmse:0.06079\n",
      "[605]\tval-rmse:0.06079\n",
      "[606]\tval-rmse:0.06079\n",
      "[607]\tval-rmse:0.06079\n",
      "[608]\tval-rmse:0.06079\n",
      "[609]\tval-rmse:0.06079\n",
      "[610]\tval-rmse:0.06079\n",
      "[611]\tval-rmse:0.06079\n",
      "[612]\tval-rmse:0.06079\n",
      "[613]\tval-rmse:0.06079\n",
      "[614]\tval-rmse:0.06079\n",
      "[615]\tval-rmse:0.06079\n",
      "[616]\tval-rmse:0.06079\n",
      "[617]\tval-rmse:0.06079\n",
      "[618]\tval-rmse:0.06079\n",
      "[619]\tval-rmse:0.06079\n",
      "[620]\tval-rmse:0.06079\n",
      "[621]\tval-rmse:0.06080\n",
      "[622]\tval-rmse:0.06079\n",
      "[623]\tval-rmse:0.06079\n",
      "[624]\tval-rmse:0.06079\n",
      "[625]\tval-rmse:0.06079\n",
      "[626]\tval-rmse:0.06079\n",
      "[627]\tval-rmse:0.06079\n",
      "[628]\tval-rmse:0.06079\n",
      "[629]\tval-rmse:0.06079\n",
      "[630]\tval-rmse:0.06079\n",
      "[631]\tval-rmse:0.06079\n",
      "[632]\tval-rmse:0.06079\n",
      "[633]\tval-rmse:0.06079\n",
      "[634]\tval-rmse:0.06079\n",
      "[635]\tval-rmse:0.06079\n",
      "[636]\tval-rmse:0.06079\n",
      "[637]\tval-rmse:0.06079\n",
      "[638]\tval-rmse:0.06079\n",
      "[639]\tval-rmse:0.06079\n",
      "[640]\tval-rmse:0.06079\n",
      "[641]\tval-rmse:0.06078\n",
      "[642]\tval-rmse:0.06078\n",
      "[643]\tval-rmse:0.06078\n",
      "[644]\tval-rmse:0.06078\n",
      "[645]\tval-rmse:0.06078\n",
      "[646]\tval-rmse:0.06078\n",
      "[647]\tval-rmse:0.06078\n",
      "[648]\tval-rmse:0.06078\n",
      "[649]\tval-rmse:0.06078\n",
      "[650]\tval-rmse:0.06078\n",
      "[651]\tval-rmse:0.06078\n",
      "[652]\tval-rmse:0.06078\n",
      "[653]\tval-rmse:0.06078\n",
      "[654]\tval-rmse:0.06078\n",
      "[655]\tval-rmse:0.06078\n",
      "[656]\tval-rmse:0.06078\n",
      "[657]\tval-rmse:0.06078\n",
      "[658]\tval-rmse:0.06078\n",
      "[659]\tval-rmse:0.06078\n",
      "[660]\tval-rmse:0.06078\n",
      "[661]\tval-rmse:0.06078\n",
      "[662]\tval-rmse:0.06078\n",
      "[663]\tval-rmse:0.06078\n",
      "[664]\tval-rmse:0.06078\n",
      "[665]\tval-rmse:0.06078\n",
      "[666]\tval-rmse:0.06078\n",
      "[667]\tval-rmse:0.06078\n",
      "[668]\tval-rmse:0.06078\n",
      "[669]\tval-rmse:0.06078\n",
      "[670]\tval-rmse:0.06078\n",
      "[671]\tval-rmse:0.06078\n",
      "[672]\tval-rmse:0.06078\n",
      "[673]\tval-rmse:0.06077\n",
      "[674]\tval-rmse:0.06077\n",
      "[675]\tval-rmse:0.06077\n",
      "[676]\tval-rmse:0.06077\n",
      "[677]\tval-rmse:0.06077\n",
      "[678]\tval-rmse:0.06077\n",
      "[679]\tval-rmse:0.06077\n",
      "[680]\tval-rmse:0.06077\n",
      "[681]\tval-rmse:0.06077\n",
      "[682]\tval-rmse:0.06077\n",
      "[683]\tval-rmse:0.06077\n",
      "[684]\tval-rmse:0.06077\n",
      "[685]\tval-rmse:0.06077\n",
      "[686]\tval-rmse:0.06077\n",
      "[687]\tval-rmse:0.06077\n",
      "[688]\tval-rmse:0.06077\n",
      "[689]\tval-rmse:0.06077\n",
      "[690]\tval-rmse:0.06077\n",
      "[691]\tval-rmse:0.06077\n",
      "[692]\tval-rmse:0.06077\n",
      "[693]\tval-rmse:0.06077\n",
      "[694]\tval-rmse:0.06076\n",
      "[695]\tval-rmse:0.06076\n",
      "[696]\tval-rmse:0.06076\n",
      "[697]\tval-rmse:0.06076\n",
      "[698]\tval-rmse:0.06076\n",
      "[699]\tval-rmse:0.06076\n",
      "[700]\tval-rmse:0.06076\n",
      "[701]\tval-rmse:0.06076\n",
      "[702]\tval-rmse:0.06076\n",
      "[703]\tval-rmse:0.06076\n",
      "[704]\tval-rmse:0.06076\n",
      "[705]\tval-rmse:0.06076\n",
      "[706]\tval-rmse:0.06076\n",
      "[707]\tval-rmse:0.06076\n",
      "[708]\tval-rmse:0.06075\n",
      "[709]\tval-rmse:0.06075\n",
      "[710]\tval-rmse:0.06075\n",
      "[711]\tval-rmse:0.06075\n",
      "[712]\tval-rmse:0.06075\n",
      "[713]\tval-rmse:0.06075\n",
      "[714]\tval-rmse:0.06075\n",
      "[715]\tval-rmse:0.06075\n",
      "[716]\tval-rmse:0.06075\n",
      "[717]\tval-rmse:0.06075\n",
      "[718]\tval-rmse:0.06075\n",
      "[719]\tval-rmse:0.06074\n",
      "[720]\tval-rmse:0.06074\n",
      "[721]\tval-rmse:0.06074\n",
      "[722]\tval-rmse:0.06074\n",
      "[723]\tval-rmse:0.06074\n",
      "[724]\tval-rmse:0.06074\n",
      "[725]\tval-rmse:0.06074\n",
      "[726]\tval-rmse:0.06073\n",
      "[727]\tval-rmse:0.06074\n",
      "[728]\tval-rmse:0.06073\n",
      "[729]\tval-rmse:0.06073\n",
      "[730]\tval-rmse:0.06073\n",
      "[731]\tval-rmse:0.06072\n",
      "[732]\tval-rmse:0.06072\n",
      "[733]\tval-rmse:0.06072\n",
      "[734]\tval-rmse:0.06072\n",
      "[735]\tval-rmse:0.06072\n",
      "[736]\tval-rmse:0.06072\n",
      "[737]\tval-rmse:0.06072\n",
      "[738]\tval-rmse:0.06072\n",
      "[739]\tval-rmse:0.06072\n",
      "[740]\tval-rmse:0.06072\n",
      "[741]\tval-rmse:0.06072\n",
      "[742]\tval-rmse:0.06072\n",
      "[743]\tval-rmse:0.06072\n",
      "[744]\tval-rmse:0.06072\n",
      "[745]\tval-rmse:0.06072\n",
      "[746]\tval-rmse:0.06071\n",
      "[747]\tval-rmse:0.06072\n",
      "[748]\tval-rmse:0.06071\n",
      "[749]\tval-rmse:0.06071\n",
      "[750]\tval-rmse:0.06071\n",
      "[751]\tval-rmse:0.06071\n",
      "[752]\tval-rmse:0.06071\n",
      "[753]\tval-rmse:0.06071\n",
      "[754]\tval-rmse:0.06071\n",
      "[755]\tval-rmse:0.06071\n",
      "[756]\tval-rmse:0.06071\n",
      "[757]\tval-rmse:0.06071\n",
      "[758]\tval-rmse:0.06071\n",
      "[759]\tval-rmse:0.06071\n",
      "[760]\tval-rmse:0.06071\n",
      "[761]\tval-rmse:0.06071\n",
      "[762]\tval-rmse:0.06071\n",
      "[763]\tval-rmse:0.06071\n",
      "[764]\tval-rmse:0.06071\n",
      "[765]\tval-rmse:0.06070\n",
      "[766]\tval-rmse:0.06071\n",
      "[767]\tval-rmse:0.06071\n",
      "[768]\tval-rmse:0.06071\n",
      "[769]\tval-rmse:0.06071\n",
      "[770]\tval-rmse:0.06070\n",
      "[771]\tval-rmse:0.06071\n",
      "[772]\tval-rmse:0.06070\n",
      "[773]\tval-rmse:0.06070\n",
      "[774]\tval-rmse:0.06070\n",
      "[775]\tval-rmse:0.06070\n",
      "[776]\tval-rmse:0.06070\n",
      "[777]\tval-rmse:0.06070\n",
      "[778]\tval-rmse:0.06069\n",
      "[779]\tval-rmse:0.06069\n",
      "[780]\tval-rmse:0.06069\n",
      "[781]\tval-rmse:0.06069\n",
      "[782]\tval-rmse:0.06069\n",
      "[783]\tval-rmse:0.06069\n",
      "[784]\tval-rmse:0.06069\n",
      "[785]\tval-rmse:0.06069\n",
      "[786]\tval-rmse:0.06069\n",
      "[787]\tval-rmse:0.06069\n",
      "[788]\tval-rmse:0.06070\n",
      "[789]\tval-rmse:0.06070\n",
      "[790]\tval-rmse:0.06070\n",
      "[791]\tval-rmse:0.06070\n",
      "[792]\tval-rmse:0.06070\n",
      "[793]\tval-rmse:0.06070\n",
      "[794]\tval-rmse:0.06070\n",
      "[795]\tval-rmse:0.06070\n",
      "[796]\tval-rmse:0.06070\n",
      "[797]\tval-rmse:0.06070\n",
      "[798]\tval-rmse:0.06070\n",
      "[799]\tval-rmse:0.06070\n",
      "[800]\tval-rmse:0.06070\n",
      "[801]\tval-rmse:0.06070\n",
      "[802]\tval-rmse:0.06070\n",
      "[803]\tval-rmse:0.06070\n",
      "[804]\tval-rmse:0.06070\n",
      "[805]\tval-rmse:0.06070\n",
      "[806]\tval-rmse:0.06070\n",
      "[807]\tval-rmse:0.06070\n",
      "[808]\tval-rmse:0.06070\n",
      "[809]\tval-rmse:0.06070\n",
      "[810]\tval-rmse:0.06070\n",
      "[811]\tval-rmse:0.06070\n",
      "[812]\tval-rmse:0.06070\n",
      "[813]\tval-rmse:0.06070\n",
      "[814]\tval-rmse:0.06070\n",
      "[815]\tval-rmse:0.06070\n",
      "[816]\tval-rmse:0.06070\n",
      "[817]\tval-rmse:0.06070\n",
      "[818]\tval-rmse:0.06070\n",
      "[819]\tval-rmse:0.06070\n",
      "[820]\tval-rmse:0.06070\n",
      "[821]\tval-rmse:0.06070\n",
      "[822]\tval-rmse:0.06070\n",
      "[823]\tval-rmse:0.06070\n",
      "[824]\tval-rmse:0.06070\n",
      "[825]\tval-rmse:0.06070\n",
      "[826]\tval-rmse:0.06070\n",
      "[827]\tval-rmse:0.06070\n",
      "[828]\tval-rmse:0.06070\n",
      "[829]\tval-rmse:0.06070\n",
      "[830]\tval-rmse:0.06070\n",
      "[831]\tval-rmse:0.06070\n",
      "[832]\tval-rmse:0.06070\n",
      "[833]\tval-rmse:0.06070\n",
      "[834]\tval-rmse:0.06070\n",
      "[835]\tval-rmse:0.06070\n",
      "[836]\tval-rmse:0.06070\n",
      "[837]\tval-rmse:0.06070\n",
      "[838]\tval-rmse:0.06070\n",
      "[839]\tval-rmse:0.06070\n",
      "[840]\tval-rmse:0.06070\n",
      "[841]\tval-rmse:0.06070\n",
      "[842]\tval-rmse:0.06070\n",
      "[843]\tval-rmse:0.06070\n",
      "[844]\tval-rmse:0.06070\n",
      "[845]\tval-rmse:0.06070\n",
      "[846]\tval-rmse:0.06070\n",
      "[847]\tval-rmse:0.06070\n",
      "[848]\tval-rmse:0.06070\n",
      "[849]\tval-rmse:0.06070\n",
      "[850]\tval-rmse:0.06070\n",
      "[851]\tval-rmse:0.06070\n",
      "[852]\tval-rmse:0.06070\n",
      "[853]\tval-rmse:0.06070\n",
      "[854]\tval-rmse:0.06070\n",
      "[855]\tval-rmse:0.06070\n",
      "[856]\tval-rmse:0.06070\n",
      "[857]\tval-rmse:0.06070\n",
      "[858]\tval-rmse:0.06070\n",
      "[859]\tval-rmse:0.06069\n",
      "[860]\tval-rmse:0.06069\n",
      "[861]\tval-rmse:0.06070\n",
      "[862]\tval-rmse:0.06069\n",
      "[863]\tval-rmse:0.06069\n",
      "[864]\tval-rmse:0.06069\n",
      "[865]\tval-rmse:0.06069\n",
      "[866]\tval-rmse:0.06069\n",
      "[867]\tval-rmse:0.06069\n",
      "[868]\tval-rmse:0.06069\n",
      "[869]\tval-rmse:0.06069\n",
      "[870]\tval-rmse:0.06069\n",
      "[871]\tval-rmse:0.06070\n",
      "[872]\tval-rmse:0.06070\n",
      "[873]\tval-rmse:0.06070\n",
      "[874]\tval-rmse:0.06070\n",
      "[875]\tval-rmse:0.06070\n",
      "[876]\tval-rmse:0.06070\n",
      "[877]\tval-rmse:0.06070\n",
      "[878]\tval-rmse:0.06070\n",
      "[879]\tval-rmse:0.06070\n",
      "[880]\tval-rmse:0.06070\n",
      "[881]\tval-rmse:0.06070\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "enc = LabelEncoder()\n",
    "predictors = ['Sex', 'Age', 'Height', 'Weight', 'Duration', 'Heart_Rate',\n",
    "       'Body_Temp']\n",
    "\n",
    "target = 'z'\n",
    "\n",
    "\n",
    "\n",
    "# Prepare the train data\n",
    "train = pd.read_csv('train.csv')\n",
    "train['Sex'] = enc.fit_transform(train['Sex'])\n",
    "train['z'] = np.log1p(train['Calories'])\n",
    "X = train[predictors]\n",
    "z = train[target]\n",
    "# 1. Initial split to create true holdout test set\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, z, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Use cross-validation to create meta-features for X_train_full\n",
    "# For each fold, train on part of the data, predict on the rest\n",
    "X_train_meta = X_train_full.copy()\n",
    "X_train_meta['z_lgb'] = np.zeros(len(X_train_full))\n",
    "X_train_meta['z_hgb'] = np.zeros(len(X_train_full))\n",
    "\n",
    "# K-fold CV for non-leaking meta-features\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_idx, val_idx in kf.split(X_train_full):\n",
    "    # Split data for this fold\n",
    "    X_fold_train, X_fold_val = X_train_full.iloc[train_idx], X_train_full.iloc[val_idx]\n",
    "    y_fold_train, y_fold_val = y_train_full.iloc[train_idx], y_train_full.iloc[val_idx]\n",
    "    \n",
    "    # Train LGB on fold's training data\n",
    "    lgb_fold = lgb.train(params_lgb, lgb.Dataset(X_fold_train, y_fold_train), num_boost_round=1000)\n",
    "    # Predict on fold's validation data (these predictions haven't seen this data)\n",
    "    (X_train_meta.iloc[val_idx])['z_lgb'] = lgb_fold.predict(X_fold_val)\n",
    "    \n",
    "    # Train HGB on fold's training data\n",
    "    hgb_fold = HistGradientBoostingRegressor(max_iter=1000,\n",
    "                                    learning_rate=0.05,\n",
    "                                    random_state=42, \n",
    "                                    early_stopping=True,\n",
    "                                    validation_fraction=0.1,\n",
    "                                    n_iter_no_change=10).fit(X_fold_train, y_fold_train)\n",
    "    # Predict on fold's validation data\n",
    "    (X_train_meta.iloc[val_idx])[ 'z_hgb'] = hgb_fold.predict(X_fold_val)\n",
    "\n",
    "# 3. Train full LGB and HGB models on ALL training data for test set predictions\n",
    "lgb_full = lgb.train(params_lgb, lgb.Dataset(X_train_full, y_train_full), num_boost_round=1000)\n",
    "hgb_full = HistGradientBoostingRegressor(max_iter=1000,\n",
    "                                    learning_rate=0.05,\n",
    "                                    random_state=42, \n",
    "                                    early_stopping=True,\n",
    "                                    validation_fraction=0.1,\n",
    "                                    n_iter_no_change=10).fit(X_train_full, y_train_full)\n",
    "\n",
    "# 4. Create meta-features for test set using full models\n",
    "X_test_meta = X_test.copy()\n",
    "X_test_meta['z_lgb'] = lgb_full.predict(X_test)\n",
    "X_test_meta['z_hgb'] = hgb_full.predict(X_test)\n",
    "\n",
    "# 5. Train second-level model (XGB) using meta-features\n",
    "predictors += ['z_lgb', 'z_hgb']\n",
    "X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(\n",
    "    X_train_meta[predictors], y_train_full, test_size=0.2, random_state=43\n",
    ")\n",
    "\n",
    "# 6. Train XGB on clean meta-features\n",
    "xgb_train = xgb.DMatrix(X_train_final, label=y_train_final)\n",
    "xgb_val = xgb.DMatrix(X_val_final, label=y_val_final)\n",
    "xgb_model = xgb.train(params_xgb, xgb_train, num_boost_round=1000, \n",
    "                     evals=[(xgb_val, 'val')], early_stopping_rounds=100)\n",
    "\n",
    "# 7. Final prediction on true holdout test set\n",
    "z_pred = xgb_model.predict(xgb.DMatrix(X_test_meta[predictors]), \n",
    "                          iteration_range=(0, xgb_model.best_iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9615631d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.0605\n"
     ]
    }
   ],
   "source": [
    "# Print RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, z_pred))\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe0a88b",
   "metadata": {},
   "source": [
    "# Generate final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75c2eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test['Sex'] = enc.transform(test['Sex'])\n",
    "test.drop(columns='id', inplace=True)\n",
    "\n",
    "z_lgb = lgb_full.predict(test)\n",
    "z_hgb = hgb_full.predict(test)\n",
    "\n",
    "\n",
    "test['z_lgb'] = z_lgb\n",
    "test['z_hgb'] = z_hgb\n",
    "# predictors += ['z_lgb', 'z_hgb']\n",
    "\n",
    "z_pred = xgb_model.predict(xgb.DMatrix(test), \n",
    "                          iteration_range=(0, xgb_model.best_iteration))\n",
    "\n",
    "y_pred = np.clip(np.expm1(z_pred), 0, None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036afc60",
   "metadata": {},
   "source": [
    "# 2-model extension of 2-layer idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c6a652b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Creating oof predictions for LGB and HGB in train set\n",
      "--Female\n",
      "--Male\n",
      "-Creating meta-features for test set using full models\n",
      "--Female\n",
      "--Male\n",
      "-Training the second-level model (XGB) using meta-features\n",
      "--Female\n",
      "--Male\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "enc = LabelEncoder()\n",
    "predictors = ['Sex', 'Age', 'Height', 'Weight', 'Duration', 'Heart_Rate',\n",
    "       'Body_Temp']\n",
    "\n",
    "target = 'z'\n",
    "\n",
    "\n",
    "\n",
    "# Prepare the train data\n",
    "train = pd.read_csv('train.csv')\n",
    "train['Sex'] = enc.fit_transform(train['Sex'])\n",
    "train['z'] = np.log1p(train['Calories'])\n",
    "X = train[predictors]\n",
    "z = train[target]\n",
    "\n",
    "# Prepare the test data\n",
    "test = pd.read_csv('test.csv')\n",
    "test['Sex'] = enc.transform(test['Sex'])\n",
    "test.drop(columns='id', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Separate datasets into male/female\n",
    "# Train\n",
    "indices_train = train['Sex']==0\n",
    "X_train_f = X[indices_train]\n",
    "y_train_f = z[indices_train]\n",
    "X_train_m = X[~indices_train]\n",
    "y_train_m = z[~indices_train]\n",
    "# Test\n",
    "indices_test = test['Sex']==0\n",
    "X_test_f = test[indices_test]\n",
    "X_test_m = test[~indices_test]\n",
    "\n",
    "# Generate oof predictions for LGB and HGB in train set\n",
    "print('-Creating oof predictions for LGB and HGB in train set')\n",
    "print('--Female')\n",
    "oof_lgb_f, oof_hgb_f = oof_generator(X_train_f, y_train_f)\n",
    "print('--Male')\n",
    "oof_lgb_m, oof_hgb_m = oof_generator(X_train_m, y_train_m)\n",
    "\n",
    "# Generate meta-features for test set using full models\n",
    "print('-Creating meta-features for test set using full models')\n",
    "print('--Female')\n",
    "lgb_test_f, hgb_test_f = generate_meta_test(X_train_f, y_train_f, X_test_f)\n",
    "print('--Male')\n",
    "lgb_test_m, hgb_test_m = generate_meta_test(X_train_m, y_train_m, X_test_m)\n",
    "\n",
    "# Prepare the final datasets\n",
    "# Train\n",
    "X_train_meta_f = X_train_f.copy()\n",
    "X_train_meta_f['z_lgb'] = oof_lgb_f\n",
    "X_train_meta_f['z_hgb'] = oof_hgb_f\n",
    "\n",
    "X_train_meta_m = X_train_m.copy()\n",
    "X_train_meta_m['z_lgb'] = oof_lgb_m\n",
    "X_train_meta_m['z_hgb'] = oof_hgb_m\n",
    "\n",
    "# Test\n",
    "X_test_meta_f = X_test_f.copy()\n",
    "X_test_meta_f['z_lgb'] = lgb_test_f\n",
    "X_test_meta_f['z_hgb'] = hgb_test_f\n",
    "\n",
    "X_test_meta_m = X_test_m.copy()\n",
    "X_test_meta_m['z_lgb'] = lgb_test_m\n",
    "X_test_meta_m['z_hgb'] = hgb_test_m\n",
    "\n",
    "# Train the second-level model (XGB) using meta-features\n",
    "print('-Training the second-level model (XGB) using meta-features')\n",
    "predictors += ['z_lgb', 'z_hgb']\n",
    "print('--Female')\n",
    "model_xgb_f = xgb.train(params_xgb, xgb.DMatrix(X_train_meta_f[predictors], label=y_train_f), num_boost_round=1000)\n",
    "print('--Male')\n",
    "model_xgb_m = xgb.train(params_xgb, xgb.DMatrix(X_train_meta_m[predictors], label=y_train_m), num_boost_round=1000)\n",
    "# Predict on the test set\n",
    "z_pred_f = model_xgb_f.predict(xgb.DMatrix(X_test_meta_f[predictors]))\n",
    "z_pred_m = model_xgb_m.predict(xgb.DMatrix(X_test_meta_m[predictors]))\n",
    "\n",
    "# Combine the predictions\n",
    "z_pred = np.zeros(len(test))\n",
    "z_pred[indices_test] = z_pred_f\n",
    "z_pred[~indices_test] = z_pred_m\n",
    "y_pred = np.clip(np.expm1(z_pred), 0, None)\n",
    "print('Done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63471cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 26.93469623, 109.09204994,  87.3721937 , 124.94670256,\n",
       "        75.85638079])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9feb3eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oof_generator(X_train, z_train):\n",
    "    \n",
    "    predictors = ['Sex', 'Age', 'Height', 'Weight', 'Duration', 'Heart_Rate',\n",
    "       'Body_Temp']\n",
    "\n",
    "    # target = 'z'\n",
    "    # 1. Initial split to create true holdout test set\n",
    "    # X_train, X_test, y_train_full, y_test = train_test_split(X_train, z_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    # 2. Use cross-validation to create meta-features for X_train\n",
    "    # For each fold, train on part of the data, predict on the rest\n",
    "    # X_train_meta = X_train.copy()\n",
    "    oof_lgb_train = np.zeros(len(X_train))\n",
    "    oof_hgb_train = np.zeros(len(X_train))\n",
    "\n",
    "    # K-fold CV for non-leaking meta-features\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        # Split data for this fold\n",
    "        X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        z_fold_train, z_fold_val = z_train.iloc[train_idx], z_train.iloc[val_idx]\n",
    "        \n",
    "        # Train LGB on fold's training data\n",
    "        lgb_fold = lgb.train(params_lgb, lgb.Dataset(X_fold_train, z_fold_train), num_boost_round=1000)\n",
    "        # Predict on fold's validation data (these predictions haven't seen this data)\n",
    "        oof_lgb_train[val_idx] = lgb_fold.predict(X_fold_val)\n",
    "        \n",
    "        # Train HGB on fold's training data\n",
    "        hgb_fold = HistGradientBoostingRegressor(max_iter=1000,\n",
    "                                        learning_rate=0.05,\n",
    "                                        random_state=42, \n",
    "                                        early_stopping=True,\n",
    "                                        validation_fraction=0.1,\n",
    "                                        n_iter_no_change=10).fit(X_fold_train, z_fold_train)\n",
    "        # Predict on fold's validation data\n",
    "        oof_hgb_train[val_idx] = hgb_fold.predict(X_fold_val)\n",
    "\n",
    "        return oof_lgb_train, oof_hgb_train\n",
    "    \n",
    "\n",
    "def generate_meta_test(X_train, y_train, X_test):\n",
    "    # 3. Train full LGB and HGB models on ALL training data for test set predictions\n",
    "    lgb_full = lgb.train(params_lgb, lgb.Dataset(X_train, y_train), num_boost_round=1000)\n",
    "    hgb_full = HistGradientBoostingRegressor(max_iter=1000,\n",
    "                                        learning_rate=0.05,\n",
    "                                        random_state=42, \n",
    "                                        early_stopping=True,\n",
    "                                        validation_fraction=0.1,\n",
    "                                        n_iter_no_change=10).fit(X_train,y_train)\n",
    "\n",
    "    # 4. Create meta-features for test set using full models\n",
    "    # X_test_meta = X_test.copy()\n",
    "\n",
    "    lgb_test = lgb_full.predict(X_test)\n",
    "    hgb_test = hgb_full.predict(X_test)\n",
    "    \n",
    "    return lgb_test, hgb_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbaaeb8",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "191c66aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission(y_pred, submission_name):\n",
    "    \n",
    "    df_submission = pd.read_csv('sample_submission.csv')\n",
    "    df_submission['Calories'] = y_pred\n",
    "    df_submission.to_csv(submission_name, index=False)\n",
    "\n",
    "submission_name = '2_layer_plus_2_model'\n",
    "generate_submission(y_pred, f'submissions\\\\{submission_name}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22ddf1f",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee1e3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interweave(array_1, array_2, indices):\n",
    "    assert len(array_1) == indices.sum()\n",
    "    output = np.zeros(len(array_1) + len(array_2))\n",
    "    output[indices] = array_1\n",
    "    output[~indices] = array_2\n",
    "    return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
